{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a testing file\n",
      "This is line2\n",
      "This is line3"
     ]
    }
   ],
   "source": [
    "!head ./example.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sukumarsubudhi/Downloads/Learning/Pyspark/pyspark-prep\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"./pyspark-prep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/27 01:26:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.224:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>count_words_app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fee5267b1f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"count_words_app\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "my_file = sc.textFile(\"/Users/sukumarsubudhi/Downloads/Learning/Pyspark/pyspark-prep/example.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method textFile in module pyspark.context:\n",
      "\n",
      "textFile(name: str, minPartitions: Union[int, NoneType] = None, use_unicode: bool = True) -> pyspark.rdd.RDD[str] method of pyspark.context.SparkContext instance\n",
      "    Read a text file from HDFS, a local file system (available on all\n",
      "    nodes), or any Hadoop-supported file system URI, and return it as an\n",
      "    RDD of Strings. The text files must be encoded as UTF-8.\n",
      "    \n",
      "    .. versionadded:: 0.7.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str\n",
      "        directory to the input data files, the path can be comma separated\n",
      "        paths as a list of inputs\n",
      "    minPartitions : int, optional\n",
      "        suggested minimum number of partitions for the resulting RDD\n",
      "    use_unicode : bool, default True\n",
      "        If `use_unicode` is False, the strings will be kept as `str` (encoding\n",
      "        as `utf-8`), which is faster and smaller than unicode.\n",
      "    \n",
      "        .. versionadded:: 1.2.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`RDD`\n",
      "        RDD representing text data from the file(s).\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    :meth:`RDD.saveAsTextFile`\n",
      "    :meth:`SparkContext.wholeTextFiles`\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import os\n",
      "    >>> import tempfile\n",
      "    >>> with tempfile.TemporaryDirectory() as d:\n",
      "    ...     path1 = os.path.join(d, \"text1\")\n",
      "    ...     path2 = os.path.join(d, \"text2\")\n",
      "    ...\n",
      "    ...     # Write a temporary text file\n",
      "    ...     sc.parallelize([\"x\", \"y\", \"z\"]).saveAsTextFile(path1)\n",
      "    ...\n",
      "    ...     # Write another temporary text file\n",
      "    ...     sc.parallelize([\"aa\", \"bb\", \"cc\"]).saveAsTextFile(path2)\n",
      "    ...\n",
      "    ...     # Load text file\n",
      "    ...     collected1 = sorted(sc.textFile(path1, 3).collect())\n",
      "    ...     collected2 = sorted(sc.textFile(path2, 4).collect())\n",
      "    ...\n",
      "    ...     # Load two text files together\n",
      "    ...     collected3 = sorted(sc.textFile('{},{}'.format(path1, path2), 5).collect())\n",
      "    \n",
      "    >>> collected1\n",
      "    ['x', 'y', 'z']\n",
      "    >>> collected2\n",
      "    ['aa', 'bb', 'cc']\n",
      "    >>> collected3\n",
      "    ['aa', 'bb', 'cc', 'x', 'y', 'z']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc.textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = my_file.flatMap(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'testing',\n",
       " 'file',\n",
       " 'This',\n",
       " 'is',\n",
       " 'line2',\n",
       " 'This',\n",
       " 'is',\n",
       " 'line3']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 words in the text file\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", str(output.count()), \"words in the text file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
