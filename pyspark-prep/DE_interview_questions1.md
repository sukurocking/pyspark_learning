1. [Convert a column with dates in the format 'MM/DD/YYYY' to a datetime object in Python.](./convert_date_column.py)
2. [Write a list comprehension which can take a table list as input and return the square of each element in the list.](./list_comprehension.py)
3. [Write a program that asks the user for input until a valid integer is entered, handling exceptions appropriately.](./python_program_for_integer.py)
4. difference between cache and broadcasting in spark and when to use which ?
5. [Transform pyspark DF data by removing all numerical and special characters, consider only alphabets](./pyspark_program_only_alphabets.ipynb)
6. Hive -
    Partitions vs Bucketing
    MSCK and REFRESH
    Add/Drop/Rename columns to the existing table 
    Parquet vs ORC vs AVRO 
7. [Spark Architecture](./Spark_DE_questionnaire.md)
   - Job/Stages/Tasks 
    Narrow and Wide Transformations
    Hive vs Spark when to use which ?
    Persist vs Cache (core difference ?)
    Cache vs Broadcast Join (core difference ?)
    Partitions Overwrite Methods (what are different available options ?)
    Shuffle Partitions (default value and how to tune it ?)
    How to submit spark jobs (write command)
    How to troubleshoot/debug Spark Job (walkthrough of Spark App History UI)
    Compression Techniques (default and difference between snappy/gzip)
    What are the benefits of Spark Adaptive Query Execution (AQE) ?
    What is Catalyst Optimizer?
    Difference between RDDs, DataFrames and Datasets
8. [Write a Python function that takes a sentence as input and returns a new sentence with the first and last words swapped.](./python_program_swap_words.py)
9.  [Given a string, replace all occurrences of the word "apple" with "orange."](./python_program_replace_word.py)
10. [Filtering a Nested List Using List Comprehension - Extracting odd numbers from Python list within list and appending them to the list](./filter_nested_list_using_list_comprehension.py)
11. [Given a list, extract the elements from index 2 to 5 using list slicing.](./list_slicing.py)
12. [Create a set with the elements "apple," "orange," and "banana." Add "grape" to the set.](./add_element_to_set.py)
13. [Create a dictionary with keys as days of the week and values as corresponding temperatures. Retrieve the temperature for "Wednesday."](./retrieve_value_from_dict.py)
14. [Write a program that asks the user for input until a valid integer is entered, handling exceptions appropriately.](./python_program_for_integer.py)
15. [Open a text file named "example.txt," read its contents, and print them.](./file_reading.py)
16. [Given a string i.e '0906455467_venkat', remove all numbers along with '_' and consider only character words using a data frame.](./string_cleaning.py)
17. [Write a function in Python that will take two arguments and replace spaces of the first argument value with the second argument.](./python_program_working_with_arguments.py)
18. [Write a function to fill missing values in a DataFrame column with the mean of that column.](./replace_missing_values_with_mean_dataframe.py)
19. Given a column with names in the format "FirstName LastName," create separate columns for first and last names.
20. Convert a column with dates in the format 'MM/DD/YYYY' to a datetime object in Python.
21. Rename a DataFrame column from 'old_name' to 'new_name.'
22. Drop columns from a DataFrame that have more than 30% missing values.
23. Calculate the average value of a numeric column grouped by a categorical column.
24. Write a function to remove punctuation and convert text to lowercase in a DataFrame column of text data.
25. [Write a Python function to check Right rotation of word and test whether 2nd word is a "right rotation" of the 1st word](./python_program_to_check_right_rotation.py)
26. [How to rename multiple columns in a Spark Dataframe?](./pyspark_program_rename_multiple_columns.py)
27. As part of Databricks certication exam for Spark developer, you should be able to complete individual data manipulation tasks
    - selecting, renaming and manipulating columns
    - filtering, sorting, dropping and aggregating rows
    - handling missing data
    - joining, reading, writing and partitioning DataFrames
    - working with UDFs and Spark SQL functions
    - the basics of the Spark architecture like execution/deployment modes, the execution hierarchy, fault tolerance, garbage collection, and broadcasting